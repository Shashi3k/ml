{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyId0VKNdNSheTwaPygsuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shashi3k/ml/blob/main/ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLNR0zhoo4TF"
      },
      "outputs": [],
      "source": [
        "import numpy as nm\n",
        "import matplotlib.pyplot as mtp\n",
        "import pandas as pd\n",
        "\n",
        "#importing datasets\n",
        "data_set= pd.read_csv('user_data.csv')\n",
        "\n",
        "#Extracting Independent and dependent Variable\n",
        "x= data_set.iloc[:, [2,3]].values\n",
        "y= data_set.iloc[:, 4].values\n",
        "\n",
        "# Splitting the dataset into training and test set.\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.25, random_state=0)\n",
        "\n",
        "#feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "st_x= StandardScaler()\n",
        "x_train= st_x.fit_transform(x_train)\n",
        "x_test= st_x.transform(x_test)\n",
        "\n",
        "From sklearn.tree import DecisionTreeClassifier\n",
        "classifier= DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        "y_pred= classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm= confusion_matrix(y_test, y_pred)\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "x_set, y_set = x_train, y_train\n",
        "x1, x2 = nm.meshgrid(nm.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1,\n",
        "step =0.01),\n",
        "nm.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))\n",
        "mtp.contourf(x1, x2, classifier.predict(nm.array([x1.ravel(),\n",
        "x2.ravel()]).T).reshape(x1.shape),\n",
        "alpha = 0.75, cmap = ListedColormap(('purple','green' )))\n",
        "mtp.xlim(x1.min(), x1.max())\n",
        "mtp.ylim(x2.min(), x2.max())\n",
        "fori, j in enumerate(nm.unique(y_set)):\n",
        "mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],\n",
        " c = ListedColormap(('purple', 'green'))(i), label = j)\n",
        "mtp.title('Decision Tree Algorithm (Training set)')\n",
        "mtp.xlabel('Age')\n",
        "mtp.ylabel('Estimated Salary')\n",
        "mtp.legend()\n",
        "mtp.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as nmp\n",
        "import matplotlib.pyplot as mpltl\n",
        "import pandas as pnd\n",
        "\n",
        "X = DS.iloc[: , 0:13].values\n",
        "Y = DS.iloc[: , 13].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "X_train, X_test, Y_train, Y_test = tts(X, Y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler as SS\n",
        "SC = SS()\n",
        "\n",
        "X_train = SC.fit_transform(X_train)\n",
        "X_test = SC.transform(X_test)\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "PCa = PCA (n_components = 1)\n",
        "X_train = PCa.fit_transform(X_train)\n",
        "X_test = PCa.transform(X_test)\n",
        "explained_variance = PCa.explained_variance_ratio_\n",
        "\n",
        "classifier_1 = LR (random_state = 0)\n",
        "classifier_1.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = classifier_1.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix as CM\n",
        "cm = CM (Y_test, Y_pred)\n",
        "\n",
        "\n",
        "from matplotlib.colors import ListedColormap as LCM\n",
        "X_set, Y_set = X_train, Y_train\n",
        "X_1, X_2 = nmp.meshgrid(nmp.arange(start = X_set[:, 0].min() - 1,\n",
        " stop = X_set[: , 0].max() + 1, step = 0.01),\n",
        " nmp.arange(start = X_set[: , 1].min() - 1,\n",
        " stop = X_set[: , 1].max() + 1, step = 0.01))\n",
        "\n",
        "mpltl.contourf(X_1, X_2, classifier_1.predict(nmp.array([X_1.ravel(),\n",
        " X_2.ravel()]).T).reshape(X_1.shape), alpha = 0.75,\n",
        " cmap = LCM (('yellow', 'grey', 'green')))\n",
        "\n",
        "mpltl.xlim (X_1.min(), X_1.max())\n",
        "mpltl.ylim (X_2.min(), X_2.max())\n",
        "\n",
        "for s, t in enumerate(nmp.unique(Y_set)):\n",
        " mpltl.scatter(X_set[Y_set == t, 0], X_set[Y_set == t, 1],\n",
        " c = LCM (('red', 'green', 'blue'))(s), label = t)\n",
        "\n",
        "mpltl.title('Logistic Regression for Training set: ')\n",
        "mpltl.xlabel ('PC_1') # for X_label\n",
        "mpltl.ylabel ('PC_2') # for Y_label\n",
        "mpltl.legend() # for showing legend\n",
        "\n",
        "# show scatter plot\n",
        "mpltl.show()"
      ],
      "metadata": {
        "id": "q2PEbL3wpVND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "st_x= StandardScaler()\n",
        "x_train= st_x.fit_transform(x_train)\n",
        "x_test= st_x.transform(x_test)\n",
        "\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier= KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )\n",
        "classifier.fit(x_train, y_train)\n",
        "\n",
        " from sklearn.metrics import confusion_matrix\n",
        " cm= confusion_matrix(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "jJ9uIFtNp1lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "classifier = SVC(kernel='linear', random_state=0)\n",
        "classifier.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "xkMK5zErqcKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= data_set.iloc[:, :-1].values\n",
        "y= data_set.iloc[:, 1].values\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "regressor= LinearRegression()\n",
        "regressor.fit(x_train, y_train)\n",
        "\n",
        "y_pred= regressor.predict(x_test)\n",
        "x_pred= regressor.predict(x_train)\n",
        "\n",
        "mtp.scatter(x_train, y_train, color=\"green\")\n",
        "mtp.plot(x_train, x_pred, color=\"red\")\n",
        "mtp.title(\"Salary vs Experience (Training Dataset)\")\n",
        "mtp.xlabel(\"Years of Experience\")\n",
        "mtp.ylabel(\"Salary(In Rupees)\")\n",
        "mtp.show()"
      ],
      "metadata": {
        "id": "thYZb47CqiG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = dataset.iloc[:, [3, 4]].values\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "wcss_list= [] #Initializing the list for the values of WCSS\n",
        " #Using for loop for iterations from 1 to 10.\n",
        "for i in range(1, 11):\n",
        " kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)\n",
        " kmeans.fit(x)\n",
        " wcss_list.append(kmeans.inertia_)\n",
        "mtp.plot(range(1, 11), wcss_list)\n",
        "mtp.title('The Elobw Method Graph')\n",
        "mtp.xlabel('Number of clusters(k)')\n",
        "mtp.ylabel('wcss_list')\n",
        "mtp.show()\n",
        "\n",
        "#training the K-means model on a dataset\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state= 42)\n",
        "y_predict= kmeans.fit_predict(x)\n",
        "y_predict\n",
        "\n",
        "mtp.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\n",
        "#for third cluster\n",
        "mtp.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster ) 4')\n",
        "#for fourth cluster\n",
        "mtp.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster)'\n",
        "5') #for fifth cluster\n",
        "mtp.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c =\n",
        "'yellow', label = 'Centroid')\n",
        "mtp.title('Clusters of customers')\n",
        "mtp.xlabel('Annual Income (k$)')\n",
        "mtp.ylabel('Spending Score (1-100)')\n",
        "mtp.legend()\n",
        "mtp.show()"
      ],
      "metadata": {
        "id": "T3fbQgAirHWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "dendro = shc.dendrogram(shc.linkage(x, method=\"ward\"))\n",
        "mtp.title(\"Dendrogrma Plot\")\n",
        "mtp.ylabel(\"Euclidean Distances\")\n",
        "mtp.xlabel(\"Customers\")\n",
        "mtp.show()\n",
        "\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc= AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')\n",
        "y_pred= hc.fit_predict(x)\n",
        "\n",
        "mtp.scatter(x[y_pred == 1, 0], x[y_pred == 1, 1], s = 100, c = 'green', label = 'Cluster 2')\n",
        "mtp.scatter(x[y_pred== 2, 0], x[y_pred == 2, 1], s = 100, c = 'red', label = 'Cluster 3')\n",
        "mtp.scatter(x[y_pred == 3, 0], x[y_pred == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4')\n",
        "mtp.scatter(x[y_pred == 4, 0], x[y_pred == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5')\n",
        "mtp.title('Clusters of customers')\n",
        "mtp.xlabel('Annual Income (k$)')\n",
        "mtp.ylabel('Spending Score (1-100)')\n",
        "mtp.legend()\n",
        "mtp.show()\n"
      ],
      "metadata": {
        "id": "jBVKYREJrhn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing numpy library as nmp\n",
        "import numpy as nmp\n",
        "# Importing pandas library as pds\n",
        "import pandas as pds\n",
        "# Importing matplotlib library as pplt\n",
        "import matplotlib.pyplot as pplt\n",
        "# Importing DBSCAN from cluster module of Sklearn library\n",
        "from sklearn.cluster import DBSCAN\n",
        "# Importing StandardSclaer and normalize from preprocessing module of Sklearn library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import normalize\n",
        "# Importing PCA from decomposition module of Sklearn\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Loading the data inside an initialized variable\n",
        "M = pds.read_csv('sampleDataset.csv') # Path of dataset file\n",
        "# Dropping the CUST_ID column from the dataset with drop() function\n",
        "M = M.drop('CUST_ID', axis = 1)\n",
        "# Using fillna() function to handle missing values\n",
        "M.fillna(method ='ffill', inplace = True)\n",
        "# Printing dataset head in output\n",
        "print(M.head())\n",
        "\n",
        "scalerFD = StandardScaler()\n",
        "# Transforming the data of dataset with Scaler\n",
        "M_scaled = scalerFD.fit_transform(M)\n",
        "# To make sure that data will follow gaussian distribution\n",
        "# We will normalize the scaled data with normalize() function\n",
        "M_normalized = normalize(M_scaled)\n",
        "# Now we will convert numpy arrays in the dataset into dataframes of panda\n",
        "M_normalized = pds.DataFrame(M_normalized)\n",
        "\n",
        "# Initializing a variable with the PCA() function\n",
        "pcaFD = PCA(n_components = 2) # components of data\n",
        "# Transforming the normalized data with PCA\n",
        "M_principal = pcaFD.fit_transform(M_normalized)\n",
        "# Making dataframes from the transformed data\n",
        "M_principal = pds.DataFrame(M_principal)\n",
        "# Creating two columns in the transformed data\n",
        "M_principal.columns = ['C1', 'C2']\n",
        "# Printing the head of the transformed data\n",
        "print(M_principal.head())\n",
        "\n",
        "# Creating clustering model of the data using the DBSCAN function and providing parameters\n",
        "in it\n",
        "db_default = DBSCAN(eps = 0.0375, min_samples = 3).fit(M_principal)\n",
        "# Labelling the clusters we have created in the dataset\n",
        "labeling = db_default.labels_\n",
        "\n",
        "# Visualization of clustering model by giving different colours\n",
        "colours = {}\n",
        "# First colour in visualization is green\n",
        "colours[0] = 'g'\n",
        "# Second colour in visualization is black\n",
        "colours[1] = 'k'\n",
        "# Third colour in visualization is red\n",
        "colours[2] = 'r'\n",
        "# Last colour in visualization is blue\n",
        "colours[-1] = 'b'\n",
        "# Creating a colour vector for each data point in the dataset cluster\n",
        "cvec = [colours[label] for label in labeling]\n",
        "# Construction of the legend\n",
        "# Scattering of green colour\n",
        "g = pplt.scatter(M_principal['C1'], M_principal['C2'], color ='g');\n",
        "# Scattering of black colour\n",
        "k = pplt.scatter(M_principal['C1'], M_principal['C2'], color ='k');\n",
        "# Scattering of red colour\n",
        "r = pplt.scatter(M_principal['C1'], M_principal['C2'], color ='r');\n",
        "# Scattering of green colour\n",
        "b = pplt.scatter(M_principal['C1'], M_principal['C2'], color ='b');\n",
        "# Plotting C1 column on the X-Axis and C2 on the Y-Axis\n",
        "# Fitting the size of the figure with figure function\n",
        "pplt.figure(figsize =(9, 9))\n",
        "# Scattering the data points in the Visualization graph\n",
        "pplt.scatter(M_principal['C1'], M_principal['C2'], c = cvec)\n",
        "# Building the legend with the coloured data points and labelled\n",
        "pplt.legend((g, k, r, b), ('Label M.0', 'Label M.1', 'Label M.2', 'Label M.-1'))\n",
        "# Showing Visualization in the output\n",
        "pplt.show()\n",
        "\n",
        "# Tuning the parameters of the model inside the DBSCAN function\n",
        "dts = DBSCAN(eps = 0.0375, min_samples = 50).fit(M_principal)\n",
        "# Labelling the clusters of data points\n",
        "labeling = dts.labels_\n",
        "\n",
        "# Labelling with different colours\n",
        "colours1 = {}\n",
        "# labelling with Red colour\n",
        "colours1[0] = 'r'\n",
        "# labelling with Green colour\n",
        "colours1[1] = 'g'\n",
        "# labelling with Blue colour\n",
        "colours1[2] = 'b'\n",
        "colours1[3] = 'c'\n",
        "# labelling with Yellow colour\n",
        "colours1[4] = 'y'\n",
        "# Magenta colour\n",
        "colours1[5] = 'm'\n",
        "# labelling with Black colour\n",
        "colours1[-1] = 'k'\n",
        "# Labelling the data points with the colour variable we have defined\n",
        "cvec = [colours1[label] for label in labeling]\n",
        "# Defining all colour that we will use\n",
        "colors = ['r', 'g', 'b', 'c', 'y', 'm', 'k' ]\n",
        "# Scattering the colours onto the data points\n",
        "r = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[0])\n",
        "g = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[1])\n",
        "b = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[2])\n",
        "c = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[3])\n",
        "y = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[4])\n",
        "m = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[5])\n",
        "k = pplt.scatter(\n",
        " M_principal['C1'], M_principal['C2'], marker ='o', color = colors[6])\n",
        "# Fitting the size of the figure with figure function\n",
        "pplt.figure(figsize =(9, 9))\n",
        "# Scattering column 1 into X-axis and column 2 into y-axis\n",
        "pplt.scatter(M_principal['C1'], M_principal['C2'], c = cvec)\n",
        "# Constructing a legend with the colours we have defined\n",
        "pplt.legend((r, g, b, c, y, m, k),\n",
        " ('Label M.0', 'Label M.1', 'Label M.2', 'Label M.3', 'Label M.4','Label M.5', 'Label M.-1'),\n",
        "# Using different labels for data points\n",
        " scatterpoints = 1, # Defining the scatter point\n",
        " loc ='upper left', # Location of cluster scattering\n",
        " ncol = 3, # Number of columns\n",
        " fontsize = 10) # Size of the font\n",
        "# Displaying the visualisation of changes in cluster scattering\n",
        "pplt.show()\n"
      ],
      "metadata": {
        "id": "yBcxBks3rxhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.compat.vi tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math, random\n",
        "\n",
        "np.random.seed(1000)\n",
        "\n",
        "function_to_learn lambda x: np.cos(x) + 0.1*np.random.randn(*x.shape)\n",
        "\n",
        "layer_1_neurons = 10\n",
        "\n",
        "NUM_points = 1000\n",
        "\n",
        "#Train the parameters of hidden layer\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "NUM_EPOCHS = 1500\n",
        "\n",
        "all_x = np.float32(np.random.uniform(-2*math.pi, 2 math.pi, (1, NUM points))).T\n",
        "\n",
        "np.random.shuffle(all_x)\n",
        "\n",
        "train_size = int(900)\n",
        "\n",
        "#Train the first 780 points in the set\n",
        "\n",
        "x_training all_x[:train_size]\n",
        "\n",
        "y_training function_to_learn(x_training)\n",
        "\n",
        "#Training the last 320 points in the given set\n",
        "\n",
        "x_validation all_x[train_size:]\n",
        "\n",
        "y_validation function_to_learn(x_validation)\n",
        "\n",
        "plt.figure(1)\n",
        "\n",
        "plt.scatter(x_training, y_training, c 'blue', label = 'train')\n",
        "\n",
        "plt.scatter(x_validation, y_validation, c= \"pink\", label 'validation')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 1], name = \"X\")\n",
        "\n",
        "Y = tf.placeholder(tf.float32, [None, 11, name = \"Y\")\n",
        "\n",
        "whtf.Variable(\n",
        "\n",
        "tf.random_uniform([1, layer_1_neurons], minval-1, maxval 1, dtype tf.float32)) b_h tf.Variable(tf.zeros([1, layer_1_neurons], dtype tf.float32))\n",
        "\n",
        "htf.nn.sigmoid(tf.matmul(X, wh) + b_h)\n",
        "\n",
        "#output layer\n",
        "\n",
        "#Number of neurons 10\n",
        "\n",
        "wo tf.Variable(\n",
        "\n",
        "tf.random_uniform([layer_1_neurons, 1], minval-1, maxval 1, dtype tf.float32)) b_o tf.Variable(tf.zeros([1, 1], dtype tf.float32))\n",
        "\n",
        "#building the model\n",
        "\n",
        "model = tf.matmul(h, w_o) + b_o\n",
        "\n",
        "#minimize the cost function (model-Y)\n",
        "\n",
        "train_op tf.train.AdamOptimizer().minimize(tf.nn.12_loss (model - Y))\n",
        "\n",
        "#Starting the Learning phase\n",
        "\n",
        "tf.Session()\n",
        "\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "errors = []\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "\n",
        "for start, end in zip(range(0, len(x_training), batch_size), range(batch_size, len(x_training), batch_size)): sess.run(train_op, feed_dict (X: x_training[start: end], V: y_training[start:end]))\n",
        "\n",
        "▸ cost = sess.run(tf.nn.12_loss (modely validation), feed_dict={X:x_validation)) errors.append(cost)\n",
        "\n",
        "if 1%1000:\n",
        "\n",
        "print(\"epoch %d, cost %g\" % (1, cost))\n",
        "\n",
        "plt.plot(errors, label='MLP Function Approximation')\n",
        "\n",
        "plt.xlabel('epochs\") plt.ylabel('cost')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FZptt0M0tlav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}